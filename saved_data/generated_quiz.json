{
  "source": "context_language",
  "mcq": [
    {
      "Q1": "What is the primary difference between image classification and object detection?",
      "A11": {
        "text": "Image classification identifies the location of objects, while object detection predicts the class of a single object.",
        "correct": false
      },
      "A12": {
        "text": "Image classification predicts the class of a single object, while object detection localizes and classifies multiple objects.",
        "correct": true
      },
      "A13": {
        "text": "Image classification uses bounding boxes, while object detection only uses class labels.",
        "correct": false
      },
      "E1": "Object detection combines both object localization (drawing bounding boxes) and image classification (identifying object classes) for multiple objects in an image, whereas image classification simply assigns a class label to a single object in an image.",
      "A14": {
        "text": "Image classification requires more computational power than object detection.",
        "correct": false
      },
      "A15": {
        "text": "Image classification combines object localization and image segmentation.",
        "correct": false
      }
    },
    {
      "Q2": "In object localization, what information does the output 'y' typically contain?",
      "A21": {
        "text": "Only the class label of the object.",
        "correct": false
      },
      "A22": {
        "text": "Only the bounding box coordinates (bx, by, bw, bh).",
        "correct": false
      },
      "A23": {
        "text": "The probability of an object's presence (pc) and the bounding box coordinates (bx, by, bw, bh).",
        "correct": true
      },
      "E2": "The output 'y' in object localization typically includes a probability (pc) indicating the presence of an object, along with the bounding box coordinates (bx, by, bw, bh) that define the object's location.",
      "A24": {
        "text": "Only the landmark coordinates.",
        "correct": false
      },
      "A25": {
        "text": "Only the dimensions of the image.",
        "correct": false
      }
    },
    {
      "Q3": "What is the purpose of Non-max Suppression in object detection?",
      "A31": {
        "text": "To increase the number of bounding boxes.",
        "correct": false
      },
      "A32": {
        "text": "To ensure that each object is detected multiple times.",
        "correct": false
      },
      "A33": {
        "text": "To clean up multiple detections of the same object by discarding overlapping bounding boxes.",
        "correct": true
      },
      "E3": "Non-max suppression eliminates redundant bounding boxes that overlap significantly, ensuring that only the most confident and accurate detections are retained for each object.",
      "A34": {
        "text": "To improve the accuracy of image classification.",
        "correct": false
      },
      "A35": {
        "text": "To train a ConvNet with cropped images.",
        "correct": false
      }
    },
    {
      "Q4": "What problem do anchor boxes solve in object detection?",
      "A41": {
        "text": "The problem of objects being too small to be detected.",
        "correct": false
      },
      "A42": {
        "text": "The problem of detecting only one object in an image.",
        "correct": false
      },
      "A43": {
        "text": "The problem of detecting multiple objects in the same grid cell.",
        "correct": true
      },
      "E4": "Anchor boxes allow a single grid cell to predict multiple objects, particularly when two objects have their midpoints within the same grid cell.",
      "A44": {
        "text": "The problem of inaccurate bounding box predictions.",
        "correct": false
      },
      "A45": {
        "text": "The problem of objects with high IoU.",
        "correct": false
      }
    },
    {
      "Q5": "In the YOLO algorithm, what does the output ùë¶= ùëù!ùëèùë•ùëèùë¶ùëè‚Ñéùëèùë§ùëê\"ùëê#ùëê$ represent when p_c = 0?",
      "A51": {
        "text": "An object is present in the grid cell, and the bounding box coordinates are relevant.",
        "correct": false
      },
      "A52": {
        "text": "An object is present in the grid cell, but only the class probabilities are relevant.",
        "correct": false
      },
      "A53": {
        "text": "No object is present in the grid cell, and all other values are irrelevant.",
        "correct": true
      },
      "E5": "When p_c = 0, it signifies that there is no object present within the grid cell. Consequently, the values for bounding box coordinates and class probabilities become irrelevant.",
      "A54": {
        "text": "An object is present, and the location refers to the background.",
        "correct": false
      },
      "A55": {
        "text": "An object is present, but all properties are unknown.",
        "correct": false
      }
    },
    {
      "Q6": "What is Intersection over Union (IoU) used for in object detection?",
      "A61": {
        "text": "Training the object detection model.",
        "correct": false
      },
      "A62": {
        "text": "Evaluating the accuracy of bounding box predictions by comparing the predicted box with the ground truth box.",
        "correct": true
      },
      "A63": {
        "text": "Generating anchor boxes.",
        "correct": false
      },
      "E6": "Intersection over Union (IoU) is a metric used to evaluate the accuracy of a bounding box prediction. It measures the overlap between the predicted bounding box and the ground truth bounding box.",
      "A64": {
        "text": "Determining the number of objects in an image.",
        "correct": false
      },
      "A65": {
        "text": "Performing non-max suppression.",
        "correct": false
      }
    },
    {
      "Q7": "Which of the following algorithms uses a segmentation algorithm to find 'blobs' before running a classifier?",
      "A71": {
        "text": "YOLO.",
        "correct": false
      },
      "A72": {
        "text": "Sliding Window Detection Algorithm.",
        "correct": false
      },
      "A73": {
        "text": "U-NET.",
        "correct": false
      },
      "E7": "R-CNN (Region with CNN) first uses a segmentation algorithm to identify potential regions of interest (blobs) in an image and then applies a classifier to each of these regions.",
      "A74": {
        "text": "R-CNN.",
        "correct": true
      },
      "A75": {
        "text": "CNN.",
        "correct": false
      }
    },
    {
      "Q8": "What is the primary goal of semantic segmentation?",
      "A81": {
        "text": "To detect the presence of objects in an image.",
        "correct": false
      },
      "A82": {
        "text": "To classify each pixel in an image with a specific class label.",
        "correct": true
      },
      "A83": {
        "text": "To draw bounding boxes around detected objects.",
        "correct": false
      },
      "E8": "Semantic segmentation aims to assign a class label to each pixel in an image, effectively creating a pixel-wise classification.",
      "A84": {
        "text": "To predict the number of objects in an image.",
        "correct": false
      },
      "A85": {
        "text": "To get rid of non-interesting region to classify.",
        "correct": false
      }
    },
    {
      "Q9": "What is the purpose of skip connections in the U-NET architecture?",
      "A91": {
        "text": "To reduce the number of channels.",
        "correct": false
      },
      "A92": {
        "text": "To increase the spatial resolution of the image.",
        "correct": false
      },
      "A93": {
        "text": "To obtain both spatial and contextual information in deeper layers.",
        "correct": true
      },
      "E9": "Skip connections in U-NET allow the network to combine high-resolution, spatial information from earlier layers with the contextual information learned in deeper layers, resulting in more precise segmentation.",
      "A94": {
        "text": "To only obtain contextual information",
        "correct": false
      },
      "A95": {
        "text": "To perform transpose convolution.",
        "correct": false
      }
    },
    {
      "Q10": "What is the primary function of transpose convolution?",
      "A101": {
        "text": "To reduce the size of a tensor.",
        "correct": false
      },
      "A102": {
        "text": "To increase the size of a tensor.",
        "correct": true
      },
      "A103": {
        "text": "To perform per-pixel classification.",
        "correct": false
      },
      "E10": "Transpose convolution is used to upsample a tensor, increasing its spatial dimensions. This is crucial in semantic segmentation for generating a segmentation map with the same dimensions as the input image.",
      "A104": {
        "text": "To identify the boundaries of objects.",
        "correct": false
      },
      "A105": {
        "text": "To use sliding windows for detection.",
        "correct": false
      }
    }
  ],
  "open_ended": [
    {
      "Q1": "Explain the difference between object localization and object detection.",
      "E1": "Object localization focuses on identifying the location of one or more objects in an image by drawing bounding boxes around them. Object detection, on the other hand, combines object localization with image classification. It not only locates the objects but also classifies them, identifying what each object is."
    },
    {
      "Q2": "Describe the sliding window detection algorithm and its primary drawback.",
      "E2": "The sliding window detection algorithm works by sliding a window of different sizes across an image and classifying the content within each window using a convolutional neural network (ConvNet). The primary drawback of this algorithm is its high computational cost, as it requires processing a large number of windows."
    },
    {
      "Q3": "Explain how the Intersection over Union (IoU) metric is used to evaluate object detection models.",
      "E3": "IoU is calculated as the area of intersection between the predicted bounding box and the ground truth bounding box, divided by the area of their union. A higher IoU indicates a better overlap and therefore a more accurate prediction. A common threshold (e.g., 0.5) is used to determine if a detection is considered \"correct.\""
    },
    {
      "Q4": "What is the purpose of anchor boxes, and how do they contribute to improving object detection?",
      "E4": "Anchor boxes are pre-defined bounding boxes with specific shapes and sizes that are used to detect multiple objects within a single grid cell. They help the model predict objects with different aspect ratios and scales, improving the detection of objects that might otherwise be missed. Each object in the training image is assigned to the grid cell that contains the object‚Äôs midpoint and anchor box for the grid cell with highest IoU."
    },
    {
      "Q5": "Explain the concept of transpose convolution and its application in semantic segmentation.",
      "E5": "Transpose convolution is an operation used to increase the spatial dimensions of a tensor. It effectively performs the reverse of a standard convolution. In semantic segmentation, transpose convolution is used in the decoding path of architectures like U-NET to upsample feature maps and generate a pixel-wise classification map with the same dimensions as the input image."
    }
  ],
  "language": "English",
  "timestamp": "2025-09-02T23:25:20.568652"
}